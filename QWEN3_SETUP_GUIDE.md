# Qwen3-4B æ¨¡å‹å®‰è£…å’Œé…ç½®æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨é…ç½®è¯­ä¹‰å·®å¼‚å·¥å…·ä½¿ç”¨ Qwen3-4B æ¨¡å‹ã€‚

## ğŸ“‹ å‰ææ¡ä»¶

- Linux ç³»ç»Ÿ
- Python 3.8+
- 4GB+ å¯ç”¨å†…å­˜
- ç½‘ç»œè¿æ¥è‰¯å¥½

## ğŸš€ å¿«é€Ÿå®‰è£…

### æ­¥éª¤ 1: å®‰è£… Ollama

```bash
# ä¸‹è½½å¹¶å®‰è£… Ollama
curl -fsSL https://ollama.com/install.sh | sh

# å¯åŠ¨ Ollama æœåŠ¡
sudo systemctl start ollama
sudo systemctl enable ollama

# éªŒè¯å®‰è£…
ollama --version
```

### æ­¥éª¤ 2: ä¸‹è½½ Qwen3-4B æ¨¡å‹

```bash
# ä¸‹è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰
ollama pull qwen3:4b

# éªŒè¯æ¨¡å‹ä¸‹è½½
ollama list
```

### æ­¥éª¤ 3: æµ‹è¯•æ¨¡å‹

```bash
# ç®€å•æµ‹è¯•
ollama run qwen3:4b "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
```

## âš™ï¸ é…ç½®è¯­ä¹‰å·®å¼‚å·¥å…·

### æ›´æ–°é…ç½®æ–‡ä»¶

ç¡®ä¿ `config.yaml` é…ç½®æ­£ç¡®ï¼š

```yaml
# æ¨¡å‹é…ç½®
model:
  # æ¨¡å‹ç±»å‹: transformers, ollama, vllm
  type: "ollama"
  
  # æ¨¡å‹åç§°
  name: "qwen3:4b"  # ollamaä¸­çš„Qwen3-4Bæ¨¡å‹åç§°
  
  # APIé…ç½® (ä»…å¯¹ ollama å’Œ vllm æœ‰æ•ˆ)
  api:
    base_url: "http://localhost:11434"  # ollamaé»˜è®¤ç«¯å£
    timeout: 30
    
  # ç”Ÿæˆå‚æ•°
  generation:
    max_length: 2048
    temperature: 0.1
    top_p: 0.9
```

### æµ‹è¯•é…ç½®

```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python3 test_qwen_api.py

# æˆ–è¿è¡Œæ¼”ç¤º
./run_demo.sh
```

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1: Ollama æœåŠ¡æ— æ³•å¯åŠ¨

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
sudo systemctl status ollama

# é‡å¯æœåŠ¡
sudo systemctl restart ollama

# æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u ollama -f
```

### é—®é¢˜ 2: æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping ollama.com

# å°è¯•é‡æ–°ä¸‹è½½
ollama rm qwen3:4b
ollama pull qwen3:4b
```

### é—®é¢˜ 3: API è¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾
netstat -tlnp | grep 11434

# æµ‹è¯• API è¿æ¥
curl http://localhost:11434/api/tags
```

### é—®é¢˜ 4: æƒé™é—®é¢˜

```bash
# æ·»åŠ ç”¨æˆ·åˆ° ollama ç»„
sudo usermod -a -G ollama $USER

# é‡æ–°ç™»å½•æˆ–åˆ·æ–°ç»„æƒé™
newgrp ollama
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### GPU åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰

```bash
# æ£€æŸ¥ NVIDIA GPU
nvidia-smi

# Ollama ä¼šè‡ªåŠ¨ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
# æ£€æŸ¥ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1
```

### å†…å­˜ä¼˜åŒ–

å¦‚æœç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š

1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼š
   ```bash
   ollama pull qwen3:1.5b
   ```
   ç„¶åæ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹åç§°ã€‚

2. è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼š
   ```yaml
   generation:
     max_length: 1024  # å‡å°‘æœ€å¤§é•¿åº¦
   ```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬å‘½ä»¤è¡Œä½¿ç”¨

```bash
# æ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶
python3 -m semantic_diff.cli.main compare examples/sample_code_old.py examples/sample_code_new.py

# äº¤äº’æ¨¡å¼
python3 -m semantic_diff.cli.main interactive

# æŸ¥çœ‹å¸®åŠ©
python3 -m semantic_diff.cli.main --help
```

### Python API ä½¿ç”¨

```python
from semantic_diff import SemanticDiff

# åˆå§‹åŒ–
diff_tool = SemanticDiff()

# æ¯”è¾ƒä»£ç 
code1 = "def hello(): return 'Hello'"
code2 = "def greet(): return 'Hi'"

result = diff_tool.compare_code(code1, code2, "python")
print(f"ç›¸ä¼¼åº¦: {result.similarity_score}")
print(f"å·®å¼‚: {len(result.differences)}")

# å…³é—­
diff_tool.shutdown()
```

## ğŸ”„ VLLM æ›¿ä»£æ–¹æ¡ˆ

å¦‚æœæ‚¨æ›´å–œæ¬¢ä½¿ç”¨ VLLMï¼š

1. å®‰è£… VLLMï¼š
   ```bash
   pip install vllm
   ```

2. å¯åŠ¨ VLLM æœåŠ¡ï¼š
   ```bash
   python -m vllm.entrypoints.openai.api_server \
     --model Qwen/Qwen3-4B-Chat \
     --served-model-name qwen3-4b \
     --port 8000
   ```

3. æ›´æ–°é…ç½®ï¼š
   ```yaml
   model:
     type: "vllm"
     name: "qwen3-4b"
     api:
       base_url: "http://localhost:8000"
   ```

## ğŸ“ é…ç½®æ–‡ä»¶è¯´æ˜

å®Œæ•´çš„é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š

```yaml
# æ¨¡å‹é…ç½®
model:
  type: "ollama"           # æ¨¡å‹ç±»å‹
  name: "qwen3:4b"         # æ¨¡å‹åç§°
  api:
    base_url: "http://localhost:11434"
    timeout: 30
  generation:
    max_length: 2048
    temperature: 0.1
    top_p: 0.9

# è¯­ä¹‰åˆ†æé…ç½®
semantic_analysis:
  depth: "medium"
  features:
    - "function_signatures"
    - "variable_names"
    - "control_flow"
    - "data_structures"
    - "comments"
    - "imports"

# è¾“å‡ºé…ç½®
output:
  format: "rich"           # plain, rich, json, html
  show_line_numbers: true
  show_context: true
  context_lines: 3

# æ€§èƒ½é…ç½®
performance:
  max_file_size: 1048576   # 1MB
  parallel_processing: true
  max_workers: 4
  cache_enabled: true

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"
  file: "semantic_diff.log"
```

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ï¼š`semantic_diff.log`
2. è¿è¡Œæµ‹è¯•ï¼š`python3 test_qwen_api.py`
3. æŸ¥çœ‹ Ollama çŠ¶æ€ï¼š`sudo systemctl status ollama`
4. æ£€æŸ¥æ¨¡å‹åˆ—è¡¨ï¼š`ollama list`

## ğŸ‰ å®Œæˆï¼

é…ç½®å®Œæˆåï¼Œæ‚¨å°±å¯ä»¥ä½¿ç”¨åŸºäº Qwen3-4B çš„è¯­ä¹‰å·®å¼‚åˆ†æå·¥å…·äº†ï¼

ä¸»è¦å‘½ä»¤ï¼š
- `./run_demo.sh` - è¿è¡Œæ¼”ç¤º
- `python3 test_qwen_api.py` - æµ‹è¯•é…ç½®
- `python3 -m semantic_diff.cli.main --help` - æŸ¥çœ‹å¸®åŠ© 